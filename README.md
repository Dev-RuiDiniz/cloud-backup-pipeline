# Cloud Backup Pipeline

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)]()
[![License: MIT](https://img.shields.io/badge/license-MIT-green.svg)]()
[![CI: GitHub Actions](https://img.shields.io/badge/CI-GitHub%20Actions-blue)]()

AutomaÃ§Ã£o completa para geraÃ§Ã£o, versionamento e armazenamento de relatÃ³rios em AWS S3 e Google Cloud Storage com logs estruturados e testes automatizados.

---

## SumÃ¡rio

- [Sobre o Projeto](#sobre-o-projeto)  
- [Principais Funcionalidades](#principais-funcionalidades)  
- [Arquitetura e Estrutura do RepositÃ³rio](#arquitetura-e-estrutura-do-repositÃ³rio)  
- [PrÃ©-requisitos](#prÃ©-requisitos)  
- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)  
- [ConfiguraÃ§Ã£o (.env)](#configuraÃ§Ã£o-env)  
- [ExecuÃ§Ã£o](#execuÃ§Ã£o)  
  - [Executar localmente (Python)](#executar-localmente-python)  
  - [Executar via script Bash](#executar-via-script-bash)  
- [GitHub Actions (CI/CD)](#github-actions-cicd)  
- [Testes](#testes)  
- [Exemplo de SaÃ­da](#exemplo-de-saÃ­da)  
- [PossÃ­veis ExtensÃµes Futuras](#possÃ­veis-extensÃµes-futuras)  
- [Tecnologias](#tecnologias)  
- [ContribuiÃ§Ã£o](#contribuiÃ§Ã£o)  
- [Autor](#autor)  
- [LicenÃ§a](#licenÃ§a)

---

## Sobre o Projeto

Este repositÃ³rio contÃ©m uma pipeline de backup em nuvem que:

- Gera relatÃ³rios (CSV e Parquet) automaticamente;
- Versiona arquivos por timestamp;
- Faz upload simultÃ¢neo para AWS S3 e Google Cloud Storage;
- MantÃ©m logs estruturados por data;
- Pode ser executada manualmente ou por GitHub Actions;
- Inclui testes automatizados (pytest) para garantir confiabilidade.

Ã‰ ideal como projeto de portfÃ³lio para Desenvolvedores Python, Engenheiros de Software e Analistas de Dados.

---

## Principais Funcionalidades

- GeraÃ§Ã£o de relatÃ³rios diÃ¡rios (CSV/Parquet).
- Upload para mÃºltiplos provedores (AWS S3 e GCS).
- Versionamento automÃ¡tico por timestamps.
- Logs rotativos e estruturados.
- ExecuÃ§Ã£o local e remota (GitHub Actions).
- Testes unitÃ¡rios com fixtures para simular uploads.

---

## Arquitetura e Estrutura do RepositÃ³rio

Raiz do projeto (resumo):
```
cloud-backup-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â””â”€â”€ output/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_reports.py
â”‚   â””â”€â”€ run_backup.sh
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ aws_service.py
â”‚   â”œâ”€â”€ gcs_service.py
â”‚   â”œâ”€â”€ backup_pipeline.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ file_utils.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_backup.py
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ daily_backup.yml
```

Breve descriÃ§Ã£o dos componentes:
- scripts/generate_reports.py â€” Gera relatÃ³rios (ex.: vendas, clientes).
- src/aws_service.py â€” Cliente/operacÃµes AWS S3 (boto3).
- src/gcs_service.py â€” Cliente/operacÃµes GCS (google-cloud-storage).
- src/backup_pipeline.py â€” Orquestra geraÃ§Ã£o, versionamento, upload e logs.
- src/utils â€” Utils (logger, manipulaÃ§Ã£o de arquivos).
- tests â€” Testes com pytest.

---

## PrÃ©-requisitos

- Python 3.8+
- Contas e credenciais AWS e Google Cloud configuradas
- pip (para instalar dependÃªncias)
- (Opcional) virtualenv/venv recomendado

---

## InstalaÃ§Ã£o

1. Clone o repositÃ³rio:
```bash
git clone https://github.com/Dev-RuiDiniz/cloud-backup-pipeline.git
cd cloud-backup-pipeline
```

2. Crie um ambiente virtual e instale dependÃªncias:
```bash
python -m venv .venv
source .venv/bin/activate   # Linux / macOS
# .venv\Scripts\activate    # Windows (PowerShell)
pip install -r requirements.txt
```

---

## ConfiguraÃ§Ã£o (.env)

Crie um arquivo `.env` na raiz do projeto com as variÃ¡veis necessÃ¡rias. Exemplo:
```env
# AWS Configuration
AWS_ACCESS_KEY_ID=SEU_AWS_ACCESS_KEY_ID
AWS_SECRET_ACCESS_KEY=SEU_AWS_SECRET_ACCESS_KEY
AWS_DEFAULT_REGION=us-east-1
AWS_S3_BUCKET=seu-bucket-s3

# Google Cloud Configuration
# Se usar GCP via variÃ¡vel de ambiente, aponte para o arquivo JSON
GOOGLE_APPLICATION_CREDENTIALS=credentials/gcp_key.json
GCS_BUCKET_NAME=seu-bucket-gcs

# Project Settings
LOG_LEVEL=INFO
```

InstruÃ§Ãµes adicionais:
- Para a GCP vocÃª pode definir a variÃ¡vel `GOOGLE_APPLICATION_CREDENTIALS` apontando para o JSON da Service Account, ou injetar o conteÃºdo via GitHub Secrets (ver seÃ§Ã£o de CI).
- Coloque o arquivo JSON em `credentials/gcp_key.json` (nÃ£o comite credenciais sensÃ­veis).

---

## ExecuÃ§Ã£o

### Executar localmente (Python)
Execute a pipeline diretamente:
```bash
python src/backup_pipeline.py
```

### Executar via script Bash
```bash
bash scripts/run_backup.sh
```

Ambas as opÃ§Ãµes geram os relatÃ³rios em `data/output/` e fazem upload para os buckets configurados.

---

## GitHub Actions (CI/CD)

O workflow `.github/workflows/daily_backup.yml` estÃ¡ preparado para rodar diariamente (cron) e contÃ©m passos para:
- Instalar dependÃªncias;
- Configurar credenciais AWS e GCP (via Secrets);
- Executar testes;
- Rodar a pipeline.

Secrets recomendados (Settings â†’ Secrets and variables â†’ Actions):
- AWS_ACCESS_KEY_ID
- AWS_SECRET_ACCESS_KEY
- AWS_DEFAULT_REGION (opcional)
- AWS_S3_BUCKET
- GCP_KEY_JSON (conteÃºdo do JSON da service account)
- GCS_BUCKET_NAME

ObservaÃ§Ã£o: o workflow deve desserializar `GCP_KEY_JSON` para um arquivo durante a execuÃ§Ã£o antes de exportar `GOOGLE_APPLICATION_CREDENTIALS`.

---

## Testes

Executar todos os testes:
```bash
pytest -v
```

Executar testes com cobertura:
```bash
pytest --cov=src --cov-report=html
```

Executar teste especÃ­fico:
```bash
pytest tests/test_backup.py -v
```

Os testes usam fixtures para simular uploads e arquivos temporÃ¡rios; verifique `tests/test_backup.py`.

---

## Exemplo de SaÃ­da

Exemplo de logs e mensagens esperadas:
```
ğŸ”„ Iniciando Pipeline de Backup - 2024-01-15 10:30:00
ğŸ“Š Gerando relatÃ³rios...
âœ… RelatÃ³rio CSV gerado: sales_report_20240115_103000.csv
âœ… RelatÃ³rio Parquet gerado: sales_report_20240115_103000.parquet
â˜ï¸ Upload para AWS S3... [SUCESSO]
ğŸ“¦ Upload para Google Cloud Storage... [SUCESSO]
ğŸ“ Log registrado em: logs/backup_2024-01-15.log
ğŸ¯ Pipeline concluÃ­da com sucesso!
```

---

## PossÃ­veis ExtensÃµes Futuras

- CompressÃ£o automÃ¡tica (gzip/zip) dos arquivos antes do upload.
- NotificaÃ§Ãµes (Slack, Telegram, e-mail) sobre o status do backup.
- API REST com FastAPI para acionar backups via HTTP.
- Dashboard em Streamlit com histÃ³rico de uploads.
- Suporte a mais provedores (Azure, Backblaze).
- OrquestraÃ§Ã£o com Airflow ou Prefect.
- PolÃ­tica de retenÃ§Ã£o automÃ¡tica de versÃµes antigas.
- Monitoramento com mÃ©tricas e alertas.

---

## Tecnologias

- Linguagem: Python 3.8+
- AWS: boto3, S3, IAM
- GCP: google-cloud-storage, Service Accounts
- Data: pandas, CSV, Parquet
- Testes: pytest
- CI/CD: GitHub Actions
- Logging: logging (rotating files)
- AutomaÃ§Ã£o: Bash, Cron

---

## ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o bem-vindas! SugestÃµes de fluxo:
1. Fork o projeto
2. Crie uma branch: `git checkout -b feature/MinhaFeature`
3. Commit suas mudanÃ§as: `git commit -m "Add: MinhaFeature"`
4. Push para a branch: `git push origin feature/MinhaFeature`
5. Abra um Pull Request

Leia e siga as boas prÃ¡ticas do repositÃ³rio e nÃ£o inclua credenciais em commits.

---

## Autor

Rui Francisco de Paula InÃ¡cio Diniz  
Engenheiro de Software | Desenvolvedor Python | Analista de Dados

GitHub: https://github.com/Dev-RuiDiniz  
LinkedIn: (link do perfil)

---

## LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT. Veja o arquivo `LICENSE` para detalhes.

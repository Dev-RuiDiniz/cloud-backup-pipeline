ğŸ“¦ Cloud Backup Pipeline
https://img.shields.io/badge/python-3.8+-blue.svg
https://img.shields.io/badge/license-MIT-green.svg
https://img.shields.io/badge/CI-GitHub%2520Actions-blue

AutomaÃ§Ã£o completa de geraÃ§Ã£o, versionamento e armazenamento de relatÃ³rios em AWS S3 e Google Cloud Storage.

ğŸ“‹ Sobre o Projeto
Este projeto implementa uma soluÃ§Ã£o completa de backup distribuÃ­do em nuvem, com geraÃ§Ã£o automÃ¡tica de relatÃ³rios, upload simultÃ¢neo para AWS S3 e Google Cloud Storage, logs estruturados, testes automatizados e execuÃ§Ã£o diÃ¡ria via GitHub Actions.

Ele foi projetado para demonstrar habilidades reais em:

Arquitetura de pipelines de dados

Python para automaÃ§Ã£o

Infraestrutura em nuvem

Desenvolvimento escalÃ¡vel e modular

Boas prÃ¡ticas de engenharia de software

OperaÃ§Ãµes contÃ­nuas com GitHub Actions

Ã‰ um projeto ideal para compor o portfÃ³lio de um Desenvolvedor Python / Engenheiro de Software / Analista de Dados em ambientes modernos de nuvem.

ğŸ¯ Objetivo do Projeto
Criar uma pipeline completa capaz de:

âœ… Gerar relatÃ³rios automaticamente (CSV e Parquet)

âœ… VersionÃ¡-los utilizando timestamps

âœ… Enviar os arquivos simultaneamente para dois provedores de nuvem:

AWS S3

Google Cloud Storage

âœ… Registrar logs estruturados por data

âœ… Ser executada manualmente ou automaticamente via GitHub Actions

âœ… Garantir confiabilidade com testes automatizados (pytest)

ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o







ğŸ“ Estrutura do Projeto
text
cloud-backup-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ input/
â”‚   â””â”€â”€ output/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_reports.py
â”‚   â”œâ”€â”€ run_backup.sh
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ aws_service.py
â”‚   â”œâ”€â”€ gcs_service.py
â”‚   â”œâ”€â”€ backup_pipeline.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py
â”‚       â””â”€â”€ file_utils.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_backup.py
â””â”€â”€ .github/
    â””â”€â”€ workflows/
        â””â”€â”€ daily_backup.yml
ğŸ§© Componentes do Sistema
1. ğŸ“Š GeraÃ§Ã£o de RelatÃ³rios
Arquivo: scripts/generate_reports.py
Gera relatÃ³rios diÃ¡rios com vendas, clientes e ticket mÃ©dio.

2. â˜ï¸ ServiÃ§o AWS S3
Arquivo: src/aws_service.py
Upload e download via Boto3.

3. ğŸ“¦ ServiÃ§o Google Cloud Storage
Arquivo: src/gcs_service.py
Upload e download via Google Cloud SDK.

4. ğŸ”„ Pipeline Principal
Arquivo: src/backup_pipeline.py
Integra toda a automaÃ§Ã£o: geraÃ§Ã£o, versionamento, logs e upload.

5. ğŸ› ï¸ UtilitÃ¡rios
Gerador de timestamps

Log estruturado por dia

Salvamento de CSV/Parquet

6. ğŸ§ª Testes
Arquivo: tests/test_backup.py
Testa upload em S3 e GCS usando fixtures.

7. âš™ï¸ AutomaÃ§Ã£o via GitHub Actions
Arquivo: .github/workflows/daily_backup.yml
Executa diariamente Ã s 03:00 AM.

8. ğŸ–¥ï¸ ExecuÃ§Ã£o Local
Arquivo: scripts/run_backup.sh

ğŸš€ Quick Start
PrÃ©-requisitos
Python 3.8+

Contas ativas na AWS e Google Cloud

Acesso programÃ¡tico configurado

â–¶ï¸ Como Executar Localmente
1. Instale as dependÃªncias
bash
pip install -r requirements.txt
2. Configure as variÃ¡veis de ambiente
Crie um arquivo .env na raiz do projeto:

env
# AWS Configuration
AWS_ACCESS_KEY_ID=seu_acesso_aqui
AWS_SECRET_ACCESS_KEY=sua_chave_secreta_aqui
AWS_DEFAULT_REGION=us-east-1
AWS_S3_BUCKET=seu-bucket-s3

# Google Cloud Configuration
GOOGLE_APPLICATION_CREDENTIALS=credentials/gcp_key.json
GCS_BUCKET_NAME=seu-bucket-gcs

# Project Settings
LOG_LEVEL=INFO
3. Configure as credenciais da Google Cloud
Crie a pasta credentials/ e adicione o arquivo JSON da service account:

bash
mkdir credentials
# Cole o arquivo JSON da service account da GCP aqui
4. Execute a pipeline
OpÃ§Ã£o 1 - Python:

bash
python src/backup_pipeline.py
OpÃ§Ã£o 2 - Script Bash:

bash
bash scripts/run_backup.sh
â˜ï¸ ExecuÃ§Ã£o na Nuvem (GitHub Actions)
A pipeline estÃ¡ configurada para execuÃ§Ã£o automÃ¡tica diÃ¡ria Ã s 03:00 UTC.

ConfiguraÃ§Ã£o necessÃ¡ria no GitHub:
Acesse Settings â†’ Secrets and variables â†’ Actions

Adicione os seguintes secrets:

Secret Name	Description
AWS_ACCESS_KEY_ID	Sua Access Key da AWS
AWS_SECRET_ACCESS_KEY	Sua Secret Access Key da AWS
GCP_KEY_JSON	ConteÃºdo JSON da service account da GCP
AWS_S3_BUCKET	Nome do bucket S3
GCS_BUCKET_NAME	Nome do bucket GCS
Fluxo do GitHub Actions:
âœ… InstalaÃ§Ã£o automÃ¡tica do ambiente Python

âœ… ConfiguraÃ§Ã£o das credenciais AWS e GCP

âœ… ExecuÃ§Ã£o dos testes automatizados

âœ… ExecuÃ§Ã£o da pipeline de backup

âœ… Logs detalhados da execuÃ§Ã£o

ğŸ§ª Testando o Projeto
Execute a suÃ­te completa de testes:

bash
# Todos os testes
pytest -v

# Testes com cobertura
pytest --cov=src --cov-report=html

# Testes especÃ­ficos
pytest tests/test_backup.py -v
ğŸ“Š Exemplo de SaÃ­da
text
ğŸ”„ Iniciando Pipeline de Backup - 2024-01-15 10:30:00
ğŸ“Š Gerando relatÃ³rios...
âœ… RelatÃ³rio CSV gerado: sales_report_20240115_103000.csv
âœ… RelatÃ³rio Parquet gerado: sales_report_20240115_103000.parquet
â˜ï¸ Upload para AWS S3... [SUCESSO]
ğŸ“¦ Upload para Google Cloud Storage... [SUCESSO]
ğŸ“ Log registrado em: logs/backup_2024-01-15.log
ğŸ¯ Pipeline concluÃ­da com sucesso!
ğŸ§  Principais Aprendizados
âœ” Arquitetura e ModularizaÃ§Ã£o AvanÃ§ada
OrganizaÃ§Ã£o do cÃ³digo em camadas de serviÃ§o, utils, scripts e testes.

âœ” Armazenamento em Nuvem
AutenticaÃ§Ã£o com AWS IAM

Uso do Boto3 para upload e download

AutenticaÃ§Ã£o com Google Service Account

UtilizaÃ§Ã£o do Google Cloud Storage client library

âœ” AutomaÃ§Ã£o com Python
CriaÃ§Ã£o de relatÃ³rios dinÃ¢micos com Pandas

ManipulaÃ§Ã£o de dados e serializaÃ§Ã£o em mÃºltiplos formatos

Versionamento automÃ¡tico por timestamps

EstruturaÃ§Ã£o de logs profissionais

âœ” DevOps e CI/CD
Pipeline automatizada com GitHub Actions

ExecuÃ§Ã£o diÃ¡ria (cron schedule)

ConfiguraÃ§Ã£o de secrets e variÃ¡veis sensÃ­veis

InstalaÃ§Ã£o de dependÃªncias e ambiente isolado no workflow

âœ” Testes Automatizados
ValidaÃ§Ã£o de rotinas de upload

Garantia de estabilidade da pipeline

Uso de fixtures e arquivos temporÃ¡rios

âœ” DocumentaÃ§Ã£o e Boas PrÃ¡ticas
README profissional

Estrutura limpa de diretÃ³rios

ComentÃ¡rios tÃ©cnicos e cÃ³digo limpo

ğŸš€ PossÃ­veis ExtensÃµes Futuras
CompressÃ£o automÃ¡tica (gzip/zip) dos arquivos

Sistema de notificaÃ§Ãµes em Slack, Telegram ou e-mail

API REST com FastAPI para acionar backups via HTTP

Dashboard em Streamlit mostrando Ãºltimos uploads

Suporte a mÃºltiplos provedores (Azure, Backblaze, Cloudflare)

OrquestraÃ§Ã£o profissional com Airflow ou Prefect

PolÃ­tica de retenÃ§Ã£o automÃ¡tica de versÃµes antigas

Monitoramento com mÃ©tricas e alertas

Interface web para gerenciamento dos backups

ğŸ› ï¸ Tecnologias Utilizadas
Categoria	Tecnologias
Linguagem	Python 3.8+
Cloud AWS	Boto3, AWS S3, IAM
Cloud Google	Google Cloud Storage, Service Accounts
Data Processing	Pandas, CSV, Parquet
Testing	Pytest, Fixtures
CI/CD	GitHub Actions, Secrets
Logging	Logging module, Rotating files
Automation	Cron, Bash scripts
ğŸ‘¨â€ğŸ’» Autor
Rui Francisco de Paula InÃ¡cio Diniz
Engenheiro de Software | Desenvolvedor Python | Analista de Dados

https://img.shields.io/badge/GitHub-Dev--RuiDiniz-black?style=flat&logo=github
https://img.shields.io/badge/LinkedIn-Perfil-blue?style=flat&logo=linkedin

ğŸ“„ LicenÃ§a
Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo LICENSE para mais detalhes.

ğŸ”„ Fluxo de Desenvolvimento
Desenvolvimento Local: Teste e validaÃ§Ã£o das funcionalidades

Commit e Push: Versionamento no GitHub

CI/CD AutomÃ¡tico: GitHub Actions executa testes

Deploy AutomÃ¡tico: Pipeline Ã© executada diariamente

Monitoramento: VerificaÃ§Ã£o dos logs e status

ğŸ¤ Contribuindo
ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para:

Fork o projeto

Criar uma branch para sua feature (git checkout -b feature/AmazingFeature)

Commit suas mudanÃ§as (git commit -m 'Add some AmazingFeature')

Push para a branch (git push origin feature/AmazingFeature)

Abrir um Pull Request

â­ Se este projeto foi Ãºtil, considere dar uma estrela no repositÃ³rio!

Este projeto foi desenvolvido para demonstrar habilidades reais em arquitetura de pipelines de dados, Python para automaÃ§Ã£o, infraestrutura em nuvem e desenvolvimento escalÃ¡vel e modular.

